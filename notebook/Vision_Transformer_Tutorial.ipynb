{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision_Transformer_Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suegy/IA-AgentScape-Rover/blob/master/notebook/Vision_Transformer_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9lOBfezfPCX"
      },
      "source": [
        "# Unofficial Walkthrough of Vision Transformer\n",
        "\n",
        "Hiroto Honda  [homepage](https://hirotomusiker.github.io/)  \n",
        "\n",
        "<a href=\"http://colab.research.google.com/github/hirotomusiker/schwert_colab_data_storage/blob/master/notebook/Vision_Transformer_Tutorial.ipynb\" target=\"_parent\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>  \n",
        "\n",
        "This notebook provides the simple walkthrough of the Vision Transformer. We hope you will be able to understand how it works by looking at the actual data flow during inference.  \n",
        "\n",
        "credits:\n",
        "- Paper: Alexey Dosovitskiy et al., \"An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\n",
        "https://arxiv.org/abs/2010.11929\n",
        "- Model Implementation: this notebook loads (and is inspired by) Ross Wightman (@wightmanr)'s amazing module: https://github.com/rwightman/pytorch-image-models/tree/master/timm . For the detailed codes, please refer to the repo.\n",
        "- The copyright of figures and demo images belongs to Hiroto Honda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONK_IcOfewkP"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_uWXBvo1X1C"
      },
      "source": [
        "%%capture\n",
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DXCF_tJ1czB"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from timm import create_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ6MYlAfDpyI"
      },
      "source": [
        "# Prepare Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koV0ey9Y1f_4"
      },
      "source": [
        "model_name = \"vit_base_patch16_224\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"device = \", device)\n",
        "# create a ViT model : https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "model = create_model(model_name, pretrained=True).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3GovnsM1t0f"
      },
      "source": [
        "# Define transforms for test\n",
        "IMG_SIZE = (224, 224)\n",
        "NORMALIZE_MEAN = (0.5, 0.5, 0.5)\n",
        "NORMALIZE_STD = (0.5, 0.5, 0.5)\n",
        "transforms = [\n",
        "              T.Resize(IMG_SIZE),\n",
        "              T.ToTensor(),\n",
        "              T.Normalize(NORMALIZE_MEAN, NORMALIZE_STD),\n",
        "              ]\n",
        "\n",
        "transforms = T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIdsKgJP6N1X"
      },
      "source": [
        "%%capture\n",
        "# ImageNet Labels\n",
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
        "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))\n",
        "\n",
        "# Demo Image\n",
        "!wget https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/santorini.png?raw=true -O santorini.png\n",
        "img = PIL.Image.open('santorini.png')\n",
        "img_tensor = transforms(img).unsqueeze(0).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI25yFRIDuI4"
      },
      "source": [
        "# Simple Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SPHZ7w83G0p"
      },
      "source": [
        "# end-to-end inference\n",
        "output = model(img_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soXYL_Yo6BIo"
      },
      "source": [
        "print(\"Inference Result:\")\n",
        "print(imagenet_labels[int(torch.argmax(output))])\n",
        "plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNlI4XUB0Gq"
      },
      "source": [
        "# Dig into Vision Transformer\n",
        "\n",
        "Let's look at the details of the Vision Transformer!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZAPoK8jBtaD"
      },
      "source": [
        "<img src='https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/vit_input.png?raw=true'>\n",
        "\n",
        "Figure 1. Vision Transformer inference pipeline.  \n",
        "1. Split Image into Patches  \n",
        "The input image is split into 14 x 14 vectors with dimension of 768 by Conv2d (k=16x16) with stride=(16, 16).\n",
        "2. Add Position Embeddings  \n",
        "Learnable position embedding vectors are added to the patch embedding vectors and fed to the transformer encoder.\n",
        "3. Transformer Encoder  \n",
        "The embedding vectors are encoded by the transformer encoder. The dimension of input and output vectors are the same. Details of the encoder are depicted in Fig. 2.\n",
        "4. MLP (Classification) Head  \n",
        "The 0th output from the encoder is fed to the MLP head for classification to output the final classification results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy8xSi3rEY4A"
      },
      "source": [
        "# 1. Split Image into Patches\n",
        "\n",
        "The input image is split into N patches (N = 14 x 14 for ViT-Base)\n",
        "and converted to D=768 embedding vectors by learnable 2D convolution:\n",
        "```\n",
        "Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWnEBkjl6iVP"
      },
      "source": [
        "patches = model.patch_embed(img_tensor)  # patch embedding convolution\n",
        "print(\"Image tensor: \", img_tensor.shape)\n",
        "print(\"Patch embeddings: \", patches.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2_kY7V8EdXG"
      },
      "source": [
        "# This is NOT a part of the pipeline.\n",
        "# Actually the image is divided into patch embeddings by Conv2d\n",
        "# with stride=(16, 16) shown above.\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "fig.suptitle(\"Visualization of Patches\", fontsize=24)\n",
        "fig.add_axes()\n",
        "img = np.asarray(img)\n",
        "for i in range(0, 196):\n",
        "    x = i % 14\n",
        "    y = i // 14\n",
        "    patch = img[y*16:(y+1)*16, x*16:(x+1)*16]\n",
        "    ax = fig.add_subplot(14, 14, i+1)\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.imshow(patch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At6eBZsqCexS"
      },
      "source": [
        "# 2. Add Position Embeddings\n",
        "To make patches position-aware, learnable 'position embedding' vectors are added to the patch embedding vectors. The position embedding vectors learn distance within the image thus neighboring ones have high similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elFemlXZXQ19"
      },
      "source": [
        "### Visualization of position embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5XhuI7UChMt"
      },
      "source": [
        "pos_embed = model.pos_embed\n",
        "print(pos_embed.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI6rRunEO6bI"
      },
      "source": [
        "# Visualize position embedding similarities.\n",
        "# One cell shows cos similarity between an embedding and all the other embeddings.\n",
        "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "fig.suptitle(\"Visualization of position embedding similarities\", fontsize=24)\n",
        "for i in range(1, pos_embed.shape[1]):\n",
        "    sim = F.cosine_similarity(pos_embed[0, i:i+1], pos_embed[0, 1:], dim=1)\n",
        "    sim = sim.reshape((14, 14)).detach().cpu().numpy()\n",
        "    ax = fig.add_subplot(14, 14, i)\n",
        "    ax.axes.get_xaxis().set_visible(False)\n",
        "    ax.axes.get_yaxis().set_visible(False)\n",
        "    ax.imshow(sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx8DuqqWaCAj"
      },
      "source": [
        "### Make Transformer Input\n",
        "A learnable class token is prepended to the patch embedding vectors as the 0th vector.  \n",
        "197 (1 + 14 x 14) learnable position embedding vectors are added to the patch embedding vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzTq9F26EHR8"
      },
      "source": [
        "transformer_input = torch.cat((model.cls_token, patches), dim=1) + pos_embed\n",
        "print(\"Transformer input: \", transformer_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3tQ2rAzX1gf"
      },
      "source": [
        "# 3. Transformer Encoder\n",
        "<img src='https://github.com/hirotomusiker/schwert_colab_data_storage/blob/master/images/vit_demo/transformer_encoder.png?raw=true'>\n",
        "\n",
        "Figure 2. Detailed schematic of Transformer Encoder.\n",
        "- N (=197) embedded vectors are fed to the L (=12) series encoders.\n",
        "- The vectors are divided into query, key and value after expanded by an fc layer.\n",
        "- q, k and v are further divided into H (=12) and fed to the parallel attention heads.\n",
        "- Outputs from attention heads are concatenated to form the vectors whose shape is the same as the encoder input.\n",
        "- The vectors go through an fc, a layer norm and an MLP block that has two fc layers.\n",
        "\n",
        "The Vision Transformer employs the Transformer Encoder that was proposed in the [attention is all you need paper](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).\n",
        "\n",
        "Implementation Reference:\n",
        "\n",
        "- [tensorflow implementation](https://github.com/google-research/vision_transformer/blob/502746cb287a107f9911c061f9d9c2c0159c81cc/vit_jax/models.py#L62-L146)\n",
        "- [pytorch implementation (timm)](https://github.com/rwightman/pytorch-image-models/blob/198f6ea0f3dae13f041f3ea5880dd79089b60d61/timm/models/vision_transformer.py#L79-L143)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kxTNNH7aXmH"
      },
      "source": [
        "### Series Transformer Encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cRyNhMQaAyh"
      },
      "source": [
        "print(\"Input tensor to Transformer (z0): \", transformer_input.shape)\n",
        "x = transformer_input.clone()\n",
        "for i, blk in enumerate(model.blocks):\n",
        "    print(\"Entering the Transformer Encoder {}\".format(i))\n",
        "    x = blk(x)\n",
        "x = model.norm(x)\n",
        "transformer_output = x[:, 0]\n",
        "print(\"Output vector from Transformer (z12-0):\", transformer_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVxGzgZE9oeY"
      },
      "source": [
        "## How Attention Works\n",
        "\n",
        "In this part, we are going to see what the actual attention looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvmVuX38ZzQF"
      },
      "source": [
        "print(\"Transformer Multi-head Attention block:\")\n",
        "attention = model.blocks[0].attn\n",
        "print(attention)\n",
        "print(\"input of the transformer encoder:\", transformer_input.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f7gQ89cvAnv"
      },
      "source": [
        "# fc layer to expand the dimension\n",
        "transformer_input_expanded = attention.qkv(transformer_input)[0]\n",
        "print(\"expanded to: \", transformer_input_expanded.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKOEQ4Nxuf3u"
      },
      "source": [
        "# Split qkv into mulitple q, k, and v vectors for multi-head attantion\n",
        "qkv = transformer_input_expanded.reshape(197, 3, 12, 64)  # (N=197, (qkv), H=12, D/H=64)\n",
        "print(\"split qkv : \", qkv.shape)\n",
        "q = qkv[:, 0].permute(1, 0, 2)  # (H=12, N=197, D/H=64)\n",
        "k = qkv[:, 1].permute(1, 0, 2)  # (H=12, N=197, D/H=64)\n",
        "kT = k.permute(0, 2, 1)  # (H=12, D/H=64, N=197)\n",
        "print(\"transposed ks: \", kT.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6mm0il6uVK6"
      },
      "source": [
        "# Attention Matrix\n",
        "attention_matrix = q @ kT\n",
        "print(\"attention matrix: \", attention_matrix.shape)\n",
        "plt.imshow(attention_matrix[3].detach().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txIf-L2Fwoxy"
      },
      "source": [
        "# Visualize attention matrix\n",
        "fig = plt.figure(figsize=(16, 8))\n",
        "fig.suptitle(\"Visualization of Attention\", fontsize=24)\n",
        "fig.add_axes()\n",
        "img = np.asarray(img)\n",
        "ax = fig.add_subplot(2, 4, 1)\n",
        "ax.imshow(img)\n",
        "for i in range(7):  # visualize the 100th rows of attention matrices in the 0-7th heads\n",
        "    attn_heatmap = attention_matrix[i, 100, 1:].reshape((14, 14)).detach().cpu().numpy()\n",
        "    ax = fig.add_subplot(2, 4, i+2)\n",
        "    ax.imshow(attn_heatmap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_hIZFmDaakU"
      },
      "source": [
        "# 4. MLP (Classification) Head\n",
        "The 0-th output vector from the transformer output vectors (corresponding to the class token input) is fed to the MLP head.  \n",
        "The 1000-dimension classification result is the output of the whole pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMLmlKflP_ax"
      },
      "source": [
        "print(\"Classification head: \", model.head)\n",
        "result = model.head(transformer_output)\n",
        "result_label_id = int(torch.argmax(result))\n",
        "plt.plot(result.detach().cpu().numpy()[0])\n",
        "plt.title(\"Classification result\")\n",
        "plt.xlabel(\"class id\")\n",
        "print(\"Inference result : id = {}, label name = {}\".format(\n",
        "    result_label_id, imagenet_labels[result_label_id]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0c_ablA67O2"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}